{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare graph data\n",
    "def prepare_graph_data(edges_file, features_file, targets_file):\n",
    "    import numpy as np\n",
    "    \n",
    "    # Load edges\n",
    "    edges_df = pd.read_csv(edges_file)\n",
    "    edge_index = torch.tensor(edges_df.values.T, dtype=torch.long)  # Shape: [2, num_edges]\n",
    "    \n",
    "    # Load features\n",
    "    with open(features_file, \"r\") as f:\n",
    "        features_json = json.load(f)\n",
    "    \n",
    "    # Map features to a consistent matrix\n",
    "    node_ids = list(map(int, features_json.keys()))  # Convert node IDs to integers\n",
    "    node_map = {node_id: i for i, node_id in enumerate(sorted(node_ids))}  # Map node IDs to indices\n",
    "    \n",
    "    # Create a feature matrix\n",
    "    num_nodes = len(node_ids)\n",
    "    max_feature_id = max(max(features) for features in features_json.values())\n",
    "    features_tensor = torch.zeros((num_nodes, max_feature_id + 1), dtype=torch.float)\n",
    "    for node_id, feature_list in features_json.items():\n",
    "        node_idx = node_map[int(node_id)]\n",
    "        features_tensor[node_idx, feature_list] = 1  # One-hot encode features\n",
    "    \n",
    "    # Load targets\n",
    "    targets_df = pd.read_csv(targets_file)\n",
    "    target_mapping = {row[\"id\"]: row[\"new_id\"] for _, row in targets_df.iterrows()}\n",
    "    target_labels = torch.zeros(num_nodes, dtype=torch.long)\n",
    "    \n",
    "    # Align targets with node indices\n",
    "    for _, row in targets_df.iterrows():\n",
    "        if row[\"id\"] in node_map:\n",
    "            target_labels[node_map[row[\"id\"]]] = 1 if row[\"mature\"] else 0  # Binary labels\n",
    "    \n",
    "    # Ensure edge indices map to node indices\n",
    "    edges_mapped = np.array([[node_map.get(src, -1), node_map.get(dst, -1)] \n",
    "                              for src, dst in edges_df.values if src in node_map and dst in node_map])\n",
    "    edges_mapped = edges_mapped[edges_mapped.min(axis=1) >= 0]  # Remove invalid edges\n",
    "    edge_index = torch.tensor(edges_mapped.T, dtype=torch.long)  # Shape: [2, num_edges]\n",
    "    \n",
    "    # Create PyTorch Geometric Data object\n",
    "    data = Data(x=features_tensor, edge_index=edge_index, y=target_labels)\n",
    "    \n",
    "    # Split data into train/val/test\n",
    "    train_idx, test_idx = train_test_split(range(len(data.y)), test_size=0.3, stratify=data.y, random_state=42)\n",
    "    val_idx, test_idx = train_test_split(test_idx, test_size=0.5, stratify=data.y[test_idx], random_state=42)\n",
    "    \n",
    "    train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    \n",
    "    train_mask[train_idx] = True\n",
    "    val_mask[val_idx] = True\n",
    "    test_mask[test_idx] = True\n",
    "    \n",
    "    data.train_mask = train_mask\n",
    "    data.val_mask = val_mask\n",
    "    data.test_mask = test_mask\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GraphSAGE model\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train_model(data, model, optimizer, criterion, epochs=200):\n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_f1 = evaluate_model(data, model, data.val_mask)\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.4f}, Val F1: {val_f1:.4f}\")\n",
    "    \n",
    "    return best_val_f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "def evaluate_model(data, model, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        preds = out[mask].argmax(dim=1)\n",
    "        f1 = f1_score(data.y[mask].cpu(), preds.cpu(), average=\"macro\")\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Epoch 10, Loss: 0.0002, Val F1: 1.0000\n",
      "Epoch 20, Loss: 0.0000, Val F1: 1.0000\n",
      "Epoch 30, Loss: 0.0000, Val F1: 1.0000\n",
      "Epoch 40, Loss: 0.0000, Val F1: 1.0000\n",
      "Epoch 50, Loss: 0.0000, Val F1: 1.0000\n",
      "Epoch 60, Loss: 0.0000, Val F1: 1.0000\n",
      "Epoch 70, Loss: 0.0000, Val F1: 1.0000\n",
      "Epoch 80, Loss: 0.0000, Val F1: 1.0000\n",
      "Epoch 90, Loss: 0.0000, Val F1: 1.0000\n",
      "Epoch 100, Loss: 0.0000, Val F1: 1.0000\n",
      "Epoch 110, Loss: 0.0000, Val F1: 1.0000\n",
      "Epoch 120, Loss: 0.0000, Val F1: 1.0000\n",
      "Epoch 130, Loss: 0.0000, Val F1: 1.0000\n",
      "Epoch 140, Loss: 0.0000, Val F1: 1.0000\n",
      "Epoch 150, Loss: 0.0000, Val F1: 1.0000\n",
      "Epoch 160, Loss: 0.0000, Val F1: 1.0000\n",
      "Epoch 170, Loss: 0.0000, Val F1: 1.0000\n",
      "Epoch 180, Loss: 0.0000, Val F1: 1.0000\n",
      "Epoch 190, Loss: 0.0000, Val F1: 1.0000\n",
      "Epoch 200, Loss: 0.0000, Val F1: 1.0000\n",
      "Testing the model...\n",
      "Test F1-score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3z/xxzlp0fj2y99cf6dk6p7zfn80000gn/T/ipykernel_52285/1906872804.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pt\"))\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # File paths\n",
    "    edges_file = \"./dataset/twitch/ENGB/musae_ENGB_edges.csv\"\n",
    "    features_file = \"./dataset/twitch/ENGB/musae_ENGB_features.json\"\n",
    "    targets_file = \"./dataset/twitch/ENGB/musae_ENGB_target.csv\"\n",
    "\n",
    "    # Prepare data\n",
    "    data = prepare_graph_data(edges_file, features_file, targets_file)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    data = data.to(device)\n",
    "\n",
    "    # Model, optimizer, and loss\n",
    "    model = GraphSAGE(in_channels=data.num_node_features, hidden_channels=64, out_channels=2).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train model\n",
    "    print(\"Training the model...\")\n",
    "    train_model(data, model, optimizer, criterion)\n",
    "\n",
    "    # Test the model\n",
    "    print(\"Testing the model...\")\n",
    "    model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "    test_f1 = evaluate_model(data, model, data.test_mask)\n",
    "    print(f\"Test F1-score: {test_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
